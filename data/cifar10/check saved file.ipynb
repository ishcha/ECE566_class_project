{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424242ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Download CIFAR-10 dataset, and splits it among clients\n",
    "\"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "\n",
    "from utils import split_dataset_by_labels, pathological_non_iid_split, split_and_reform_dataset\n",
    "\n",
    "ALPHA = .4\n",
    "N_CLASSES = 10\n",
    "N_COMPONENTS = 3\n",
    "SEED = 12345\n",
    "RAW_DATA_PATH = \"raw_data/\"\n",
    "PATH = \"all_data/\"\n",
    "\n",
    "\n",
    "def save_data(l, path_):\n",
    "    with open(path_, 'wb') as f:\n",
    "        pickle.dump(l, f)\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--n_tasks',\n",
    "        help='number of tasks/clients;',\n",
    "        type=int,\n",
    "        required=True)\n",
    "    parser.add_argument(\n",
    "        '--pathological_split',\n",
    "        help='if selected, the dataset will be split as in'\n",
    "             '\"Communication-Efficient Learning of Deep Networks from Decentralized Data\";'\n",
    "             'i.e., each client will receive `n_shards` of dataset, where each shard contains at most two classes',\n",
    "        action='store_true'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--n_shards',\n",
    "        help='number of shards given to each clients/task; ignored if `--pathological_split` is not used;'\n",
    "             'default is 2',\n",
    "        type=int,\n",
    "        default=2\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--n_components',\n",
    "        help='number of components/clusters;',\n",
    "        type=int,\n",
    "        default=N_COMPONENTS\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--alpha',\n",
    "        help='parameter controlling tasks dissimilarity, the smaller alpha is the more tasks are dissimilar;',\n",
    "        type=float,\n",
    "        default=ALPHA\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--s_frac',\n",
    "        help='fraction of the dataset to be used; default: 1.0;',\n",
    "        type=float,\n",
    "        default=1.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--tr_frac',\n",
    "        help='fraction in training set; default: 0.8;',\n",
    "        type=float,\n",
    "        default=0.8\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--val_frac',\n",
    "        help='fraction of validation set (from train set); default: 0.0;',\n",
    "        type=float,\n",
    "        default=0.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--test_tasks_frac',\n",
    "        help='fraction of tasks / clients not participating to the training; default is 0.0',\n",
    "        type=float,\n",
    "        default=0.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--seed',\n",
    "        help='seed for the random processes;',\n",
    "        type=int,\n",
    "        default=SEED\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--distribution_shift',\n",
    "        help='if selected, the dataset will be split such that each component',\n",
    "        action='store_true'\n",
    "    )\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "# python generate_data.py \\\n",
    "#     --n_tasks 80 \\\n",
    "#     --n_components -1 \\\n",
    "#     --alpha 0.5 \\\n",
    "#     --s_frac 1.0 \\\n",
    "#     --tr_frac 0.8 \\\n",
    "#     --val_frac 0.25 \\\n",
    "#     --seed 12345 --distribution_shift  \n",
    "def main():\n",
    "    args = parse_args(\"python generate_data.py \\\n",
    "    --n_tasks 80 \\\n",
    "    --n_components -1 \\\n",
    "    --alpha 0.5 \\\n",
    "    --s_frac 1.0 \\\n",
    "    --tr_frac 0.8 \\\n",
    "    --val_frac 0.25 \\\n",
    "    --seed 12345 --distribution_shift  \")\n",
    "\n",
    "    transform = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "\n",
    "    dataset =\\\n",
    "        ConcatDataset([\n",
    "            CIFAR10(root=RAW_DATA_PATH, download=True, train=True, transform=transform),\n",
    "            CIFAR10(root=RAW_DATA_PATH, download=False, train=False, transform=transform)\n",
    "        ])\n",
    "\n",
    "    if args.pathological_split:\n",
    "        clients_indices = \\\n",
    "            pathological_non_iid_split(\n",
    "                dataset=dataset,\n",
    "                n_classes=N_CLASSES,\n",
    "                n_clients=args.n_tasks,\n",
    "                n_classes_per_client=args.n_shards,\n",
    "                frac=args.s_frac,\n",
    "                seed=args.seed\n",
    "            )\n",
    "    elif args.distribution_shift:\n",
    "        clients_indices, rotation_idx = \\\n",
    "            split_and_reform_dataset(\n",
    "                dataset=dataset,\n",
    "                n_classes=N_CLASSES,\n",
    "                n_clients=args.n_tasks,\n",
    "                n_clusters=args.n_components,\n",
    "                alpha=args.alpha,\n",
    "                frac=args.s_frac,\n",
    "                seed=args.seed,\n",
    "                rotation_ratio=0.7\n",
    "            )\n",
    "    else:\n",
    "        clients_indices = \\\n",
    "            split_dataset_by_labels(\n",
    "                dataset=dataset,\n",
    "                n_classes=N_CLASSES,\n",
    "                n_clients=args.n_tasks,\n",
    "                n_clusters=args.n_components,\n",
    "                alpha=args.alpha,\n",
    "                frac=args.s_frac,\n",
    "                seed=args.seed\n",
    "            )\n",
    "\n",
    "    if args.test_tasks_frac > 0:\n",
    "        train_clients_indices, test_clients_indices = \\\n",
    "            train_test_split(clients_indices, test_size=args.test_tasks_frac, random_state=args.seed)\n",
    "    else:\n",
    "        train_clients_indices, test_clients_indices = clients_indices, []\n",
    "\n",
    "    os.makedirs(os.path.join(PATH, \"train\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(PATH, \"test\"), exist_ok=True)\n",
    "    \"\"\"\n",
    "    \" require significant shifting\n",
    "    \"\"\"\n",
    "    if args.distribution_shift:\n",
    "        save_data(rotation_idx, os.path.join(PATH, \"rotations.pkl\"))\n",
    "\n",
    "    for mode, clients_indices in [('train', train_clients_indices), ('test', test_clients_indices)]:\n",
    "        for client_id, indices in enumerate(clients_indices):\n",
    "            if len(indices) == 0:\n",
    "                continue\n",
    "\n",
    "            client_path = os.path.join(PATH, mode, \"task_{}\".format(client_id))\n",
    "            os.makedirs(client_path, exist_ok=True)\n",
    "\n",
    "            train_indices, test_indices = \\\n",
    "                train_test_split(\n",
    "                    indices,\n",
    "                    train_size=args.tr_frac,\n",
    "                    random_state=args.seed\n",
    "                )\n",
    "\n",
    "            if args.val_frac > 0:\n",
    "                train_indices, val_indices = \\\n",
    "                    train_test_split(\n",
    "                        train_indices,\n",
    "                        train_size=1.-args.val_frac,\n",
    "                        random_state=args.seed\n",
    "                    )\n",
    "\n",
    "                save_data(val_indices, os.path.join(client_path, \"val.pkl\"))\n",
    "\n",
    "            print(len(train_indices), len(test_indices))\n",
    "            save_data(train_indices, os.path.join(client_path, \"train.pkl\"))\n",
    "            save_data(test_indices, os.path.join(client_path, \"test.pkl\"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a767385",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --n_tasks N_TASKS [--pathological_split]\n",
      "                             [--n_shards N_SHARDS]\n",
      "                             [--n_components N_COMPONENTS] [--alpha ALPHA]\n",
      "                             [--s_frac S_FRAC] [--tr_frac TR_FRAC]\n",
      "                             [--val_frac VAL_FRAC]\n",
      "                             [--test_tasks_frac TEST_TASKS_FRAC] [--seed SEED]\n",
      "                             [--distribution_shift]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --n_tasks\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zshuai8/anaconda3/envs/fedml/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3406: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "ALPHA = .4\n",
    "N_CLASSES = 10\n",
    "N_COMPONENTS = 3\n",
    "SEED = 12345\n",
    "RAW_DATA_PATH = \"raw_data/\"\n",
    "PATH = \"all_data/\"\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--n_tasks',\n",
    "        help='number of tasks/clients;',\n",
    "        type=int,\n",
    "        required=True)\n",
    "    parser.add_argument(\n",
    "        '--pathological_split',\n",
    "        help='if selected, the dataset will be split as in'\n",
    "             '\"Communication-Efficient Learning of Deep Networks from Decentralized Data\";'\n",
    "             'i.e., each client will receive `n_shards` of dataset, where each shard contains at most two classes',\n",
    "        action='store_true'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--n_shards',\n",
    "        help='number of shards given to each clients/task; ignored if `--pathological_split` is not used;'\n",
    "             'default is 2',\n",
    "        type=int,\n",
    "        default=2\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--n_components',\n",
    "        help='number of components/clusters;',\n",
    "        type=int,\n",
    "        default=N_COMPONENTS\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--alpha',\n",
    "        help='parameter controlling tasks dissimilarity, the smaller alpha is the more tasks are dissimilar;',\n",
    "        type=float,\n",
    "        default=ALPHA\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--s_frac',\n",
    "        help='fraction of the dataset to be used; default: 1.0;',\n",
    "        type=float,\n",
    "        default=1.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--tr_frac',\n",
    "        help='fraction in training set; default: 0.8;',\n",
    "        type=float,\n",
    "        default=0.8\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--val_frac',\n",
    "        help='fraction of validation set (from train set); default: 0.0;',\n",
    "        type=float,\n",
    "        default=0.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--test_tasks_frac',\n",
    "        help='fraction of tasks / clients not participating to the training; default is 0.0',\n",
    "        type=float,\n",
    "        default=0.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--seed',\n",
    "        help='seed for the random processes;',\n",
    "        type=int,\n",
    "        default=SEED\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--distribution_shift',\n",
    "        help='if selected, the dataset will be split such that each component',\n",
    "        action='store_true'\n",
    "    )\n",
    "\n",
    "    return parser.parse_args([\"python generate_data.py \\\n",
    "    --n_tasks 80 \\\n",
    "    --n_components -1 \\\n",
    "    --alpha 0.5 \\\n",
    "    --s_frac 1.0 \\\n",
    "    --tr_frac 0.8 \\\n",
    "    --val_frac 0.25 \\\n",
    "    --seed 12345 --distribution_shift\"])\n",
    "args = parse_args()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1cf9e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = open(\"all_data/rotations.pkl\",'rb')\n",
    "object_file = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6561eb61",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "img should be PIL Image. Got <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m augmented \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     28\u001b[0m augmented\u001b[38;5;241m.\u001b[39mappend(imgs)\n\u001b[0;32m---> 29\u001b[0m imgs \u001b[38;5;241m=\u001b[39m \u001b[43mgray_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m augmented\u001b[38;5;241m.\u001b[39mappend(imgs)\n\u001b[1;32m     31\u001b[0m imgs \u001b[38;5;241m=\u001b[39m jitter(imgs)\n",
      "File \u001b[0;32m~/anaconda3/envs/fedml/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/fedml/lib/python3.10/site-packages/torchvision/transforms/transforms.py:1560\u001b[0m, in \u001b[0;36mGrayscale.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m   1553\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   1555\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be converted to grayscale.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Grayscaled image.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1560\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrgb_to_grayscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_output_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_output_channels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/fedml/lib/python3.10/site-packages/torchvision/transforms/functional.py:1277\u001b[0m, in \u001b[0;36mrgb_to_grayscale\u001b[0;34m(img, num_output_channels)\u001b[0m\n\u001b[1;32m   1275\u001b[0m     _log_api_usage_once(rgb_to_grayscale)\n\u001b[1;32m   1276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m-> 1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_grayscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_output_channels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mrgb_to_grayscale(img, num_output_channels)\n",
      "File \u001b[0;32m~/anaconda3/envs/fedml/lib/python3.10/site-packages/torchvision/transforms/functional_pil.py:336\u001b[0m, in \u001b[0;36mto_grayscale\u001b[0;34m(img, num_output_channels)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39munused\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_grayscale\u001b[39m(img: Image\u001b[38;5;241m.\u001b[39mImage, num_output_channels: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Image\u001b[38;5;241m.\u001b[39mImage:\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_pil_image(img):\n\u001b[0;32m--> 336\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be PIL Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(img)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_output_channels \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    339\u001b[0m         img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: img should be PIL Image. Got <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "import os\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "cifar10_path = os.path.join(\"raw_data\")\n",
    "assert os.path.isdir(cifar10_path), \"Download cifar10 dataset!!\"\n",
    "cifar10_train = \\\n",
    "        CIFAR10(\n",
    "            root=cifar10_path,\n",
    "            train=True, download=False\n",
    "        )\n",
    "\n",
    "with open('all_data/rotations.pkl', \"rb\") as f:\n",
    "    rotation_idx = pickle.load(f)\n",
    "\n",
    "    x = cifar10_train.data[rotation_idx[0][:1000]]\n",
    "#         y = cifar10_targets[rotation_idx]\n",
    "    rotater = T.RandomRotation(degrees=(0, 180))\n",
    "    jitter = T.ColorJitter(brightness=.5, hue=.3)\n",
    "\n",
    "    gray_img = T.Grayscale()\n",
    "    jitter = T.ColorJitter(brightness=.5, hue=.3)\n",
    "    inverter = T.RandomInvert()\n",
    "    rotater = T.RandomRotation(degrees=(0, 180))\n",
    "    imgs = x\n",
    "    augmented = []\n",
    "\n",
    "    augmented.append(imgs)\n",
    "    imgs = gray_img(imgs)\n",
    "    augmented.append(imgs)\n",
    "    imgs = jitter(imgs)\n",
    "    augmented.append(imgs)\n",
    "    imgs = inverter(imgs)\n",
    "    augmented.append(imgs)\n",
    "    imgs = rotater(imgs)\n",
    "    augmented.append(imgs)\n",
    "\n",
    "#         y = permute_label(y, 10)\n",
    "\n",
    "#         cifar10_data[rotation_idx] = x\n",
    "#         cifar10_targets[rotation_idx] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3aab910f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pic should be Tensor or ndarray. Got <class 'torchvision.datasets.cifar.CIFAR10'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m      2\u001b[0m transform \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      3\u001b[0m         T\u001b[38;5;241m.\u001b[39mToPILImage(),\n\u001b[1;32m      4\u001b[0m         T\u001b[38;5;241m.\u001b[39mToTensor()])\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcifar10_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/fedml/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/fedml/lib/python3.10/site-packages/torchvision/transforms/transforms.py:227\u001b[0m, in \u001b[0;36mToPILImage.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m        pic (Tensor or numpy.ndarray): Image to be converted to PIL Image.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m \n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pil_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/fedml/lib/python3.10/site-packages/torchvision/transforms/functional.py:259\u001b[0m, in \u001b[0;36mto_pil_image\u001b[0;34m(pic, mode)\u001b[0m\n\u001b[1;32m    256\u001b[0m     _log_api_usage_once(to_pil_image)\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(pic, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pic, np\u001b[38;5;241m.\u001b[39mndarray)):\n\u001b[0;32m--> 259\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpic should be Tensor or ndarray. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(pic)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pic, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m}:\n",
      "\u001b[0;31mTypeError\u001b[0m: pic should be Tensor or ndarray. Got <class 'torchvision.datasets.cifar.CIFAR10'>."
     ]
    }
   ],
   "source": [
    "import torchvision.transforms.functional as F\n",
    "transform = T.Compose([\n",
    "        T.ToPILImage(),\n",
    "        T.ToTensor()])\n",
    "\n",
    "transform(cifar10_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedml",
   "language": "python",
   "name": "fedml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
